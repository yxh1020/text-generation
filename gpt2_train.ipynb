{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezlDhRGgiQo7"
   },
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tT0YSCPJiJFQ"
   },
   "outputs": [],
   "source": [
    "!pip install -q gpt-2-simple\n",
    "import gpt_2_simple as gpt2\n",
    "from datetime import datetime\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NRjs9222igN8"
   },
   "source": [
    "# Downloading GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16572,
     "status": "ok",
     "timestamp": 1573582008186,
     "user": {
      "displayName": "Yuan Hu",
      "photoUrl": "",
      "userId": "14204862805009818583"
     },
     "user_tz": 480
    },
    "id": "R0_2uS8BiUPD",
    "outputId": "b31e596b-c7c3-42c3-b011-2864e60ba8c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 303Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 73.2Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 368Mit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:05, 84.2Mit/s]                                  \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 273Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 93.3Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 143Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "gpt2.download_gpt2(model_name=\"124M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbBPpdxzi96V"
   },
   "source": [
    "# Mounting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 763,
     "status": "ok",
     "timestamp": 1573587768514,
     "user": {
      "displayName": "Yuan Hu",
      "photoUrl": "",
      "userId": "14204862805009818583"
     },
     "user_tz": 480
    },
    "id": "R-sJDMjtiqCD",
    "outputId": "e453b63d-5299-4f34-9bc3-952769f87c45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "gpt2.mount_gdrive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dA_AMG5ojoL5"
   },
   "outputs": [],
   "source": [
    "file_name = 'nips_clean.txt'\n",
    "gpt2.copy_file_from_gdrive(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6u6nf_zpU_q"
   },
   "source": [
    "# Finetune GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9343392,
     "status": "ok",
     "timestamp": 1573597125764,
     "user": {
      "displayName": "Yuan Hu",
      "photoUrl": "",
      "userId": "14204862805009818583"
     },
     "user_tz": 480
    },
    "id": "UrN_vp7YnrLL",
    "outputId": "78e1204d-d742-4e0e-e19e-80d8b992fcf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Loading checkpoint checkpoint/run1/model-1000\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:05<00:00,  5.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 860454 tokens\n",
      "Training...\n",
      "Saving checkpoint/run1/model-1000\n",
      "Saving checkpoint/run1/model-1000\n",
      "======== SAMPLE 1 ========\n",
      " with which we can identify the distribution specific functional elements of data induced by the discriminant . the proposed model outperforms known discriminant models in data acquisition , in terms of interpretability and in terms of reducing sample complexity of learning the discriminant . the method is generalized to new instances .\n",
      "we propose a scalable optimization procedure to learn the gradient of a nonlinear function f over d dimensional vectors g . our proposed algorithm first calculates the integral and gradient of g at run time d and then distributes the gradients across the d dimensions . we demonstrate how this procedure can be used to learn the gradient in a simple NUMBER bit integer program . our proposed algorithm is scalable as we first establish the consistency of our approximation formulation to known polynomial objectives . the size of our program depends on the dimension of g and d , as well as the dimension of d and g . finally , we prove that the parameter space of our algorithm attains an optimal convergence rate of o and a logarithmic rate of the log likelihood . we observe many applications using this optimization procedure on dimension reduction and network estimation .\n",
      "many deep learning tasks , such as vision and textual description generation , require identifying the most relevant content for each image and video segment . given a high level visual representation , it is critical to identify the most relevant part of that representation . in many cases however , it may be difficult to achieve such high level visual expertise with small training sets for image classification and scene segmentation tasks . in such cases , multi class convolutional neural networks have emerged as one effective approach to addressing this challenging task . however , as our unified framework makes our contribution accessible to a wide variety of datasets and architectures , this effort provides a natural framework for understanding how convolutional neural networks and their corresponding local processing units relate to scene understanding . using this framework , we develop a new local scene segmentation method which includes a simple gaussian process in its core . this gcm is trained end to end to produce visual segments that are well aligned for classification in scenes with relevant part representations . we apply the proposed local scene segmentation to dozens of cases of image classification , and show that the proposed local gccn approach can generate visually revealing images after just a few hundred inputs with a small training set .\n",
      "in a multi task learning framework , we seek to identify the most relevant subset of tasks that can be correctly classified into a given set of tasks . we formulate this task as a generalization of a multi task ranking problem , namely that of ranking the tasks in a given set of constraints . we propose an efficient online multi task prediction algorithm based on a modified version of the celebrated svrg NUMBER algorithm . however , the regularized svrg algorithm performs worse against a number of strong baselines than the fully supervised svrg NUMBER algorithm . as the problem of identifying the most relevant subset of tasks is extremely complex and has an inherent non convexity , this new multi task ranking method provides an efficient online algorithm that is robust to o random perturbations . we also present an optimistic version of the svrg algorithm that is adaptive to the underlying constraints , and we evaluate the effectiveness of the proposed algorithm with empirical evaluation of multi task prediction problems from NUMBER usersbooks .\n",
      "we address the challenge of identifying generalization risk minimization problems , where a learner has access to a model and the learning objective is to estimate a function that minimizes the expected loss in prediction . we define a general formulation with bounded eigenvalue that allows for a simple alternating optimization problem , and in addition we provide a lower bound on the generalization error . while the framework in question is a generalization error bound based on model selection , we show that a much simpler one that applies only to domain models may have suboptimal performance . we provide experimental results on both empirical and game playing data demonstrating that the benefits of the domain models are indeed remarkable .\n",
      "in this paper we study the non stationarity hypothesis for continuous time markov decision processes , which shows that under sufficiently strong conditions in which the system is sampled one can acquire stationary probability densities with no excess risk . we show this for some basic types of mdp and the non stationarity dimension hochstrap mdp , where the non stationarity parameter is the sum of latent and observable quantities , and the value of the system is determined by the intrinsic entropy value function . this is of independent interest and in many cases of extreme practicality incompatible with the theoretical assumption of independence which governs the coherence of the mdp .\n",
      "a non negative bag of healers is a widely used method to estimate the robustness of neural networks under model misspecified perturbations . although there exist several models that combine robustness and robustness predictions of several classes , recent advances in bayesian statistical inference have enabled bayesian statistical models to be explicitly interpretable , particularly for unbounded perturbations of the data . however , the representational difficulty of robust models has so far eluded a kernel based model or kernel bayes . in this paper , we present a first order kernel based residual\n",
      "\n",
      "[1050 | 256.95] loss=2.14 avg=2.14\n",
      "[1100 | 484.67] loss=2.15 avg=2.15\n",
      "[1150 | 713.06] loss=1.92 avg=2.07\n",
      "[1200 | 941.12] loss=2.01 avg=2.06\n",
      "======== SAMPLE 1 ========\n",
      " the previous work , which found that NUMBER penalized mzmt can approximate more complex kernel functions via NUMBER training the NUMBER mzmt kernel with a non smooth NUMBER loss . we show that , by taking the convex geometry of kernels matrices and adding regularity operators on the smoothness of these kernels , one can reduce the complexity of this problem considerably by learning a non smooth NUMBER loss . this provides the first provable solution of a multi stage learning problem , that involves a linear time linearized mzmt algorithm based on convex relaxation .\n",
      "we give a fast , scalable and memory stable algorithm for the stochastic approximation to positive definite ma et als empirical study of matrix approximations , using real valued data in pathological constraints . the algorithms output is more compact and more stable than existing algorithms such as lp algorithm , which has runtime in hours on the go or system memory . our algorithm is simpler , runs much better and relies on a simple optimization problem to trade accuracy for computation time . importantly , we do not need any knowledge of the problem at all . we give several examples from the stochastic approximation literature which explore this algorithmic choice in the large scale application environment of real life problems . experiments on four real world data sets indicate the effectiveness and efficiency of our algorithm in reducing pathological data to pathological data .\n",
      "we propose a black box framework for estimating the mutual information of two or more individuals based on their visual motion cues . we describe a generative model for the visual motion cues , using motions as joint measures of information , so as to approximate the mutual information between the two individual cues . the model captures both spatial correlations in the joint motion information of the two individuals , and also differences in the context in which they are viewed . by construction , our model approximates the mutual information in the two individuals , so that the mutual information in the one individual can be accurately estimated . in addition , we show that estimating the mutual information in isolation can greatly improve the quality of recall obtained with a single trial , given the same amount of background stimuli . in simulations of natural images , we show that our approach outperforms state of the art methods in the tasks used as reference data , especially for the detection of motion cues .\n",
      "generative adversarial networks have recently achieved impressive results on various tasks . however , their computational complexity requires that they are fully trained so that data is invariant to the given generative model . this requires fine tuning the structure to allow training that utilizes a learned model for many of the tasks . we propose a simple system that automatically adjusts the weights between training examples for each task using iterative backpropagation . the new architecture is computationally light and very promising on several models of image generation , such as scene classification , generative convolution , etc . we test the model on several popular tasks , on which we obtain state of the art performance on at least one model , and show that the model learns to extract invariant networks that perform very well on other models , even on tasks that were not studied in this paper .\n",
      "the sparse inverse covariance estimation task has attracted much attention in statistics and machine learning . it allows sampling from a logarithmic distribution , which is a popular target for non degenerate gaussian distributions . in this work , we prove a simple and practical guarantee for an approximate local covariance estimation algorithm on smooth logit distributions under mild conditions , and a stronger but nonsmooth condition for the strong unbounded approximation . this guarantee applies to nonapprox personalized data estimation . we prove that , under a milder structural assumption , our method , when applied to the inverse covariance estimation problem , provides strong empirical evidence that our methods , as a class , come equipped with a compelling convergence theory . our proof involves a graph embedding of the risk function of the approximate local covariance levy on a spherical coordinate system , and leads straight to a comprehensive numerical analysis .\n",
      "we propose a scalable , theoretical study of the stochastic approximation to the posterior using random pairs . we cast the problem as a product learning problem paired with a discrete function as a learning rate function . within the span of an optimal random pair , this learning function can also be a function that is optimized by an auxiliary function to make gradient predictions . we devise an efficient algorithm based on the notion of the eulerian markov chain , and establish theoretical consistency for a novel approximation to the posterior using random pairs . in experiments on the challenging mnist and cifar NUMBER , we demonstrate a significant performance improvement over the current state of the art methods .\n",
      "recent advances in statistical analysis have led to substantial advances in the field of unsupervised learning . in this paper , we address the problem of unsupervised learning using unsupervised statistical models defined on clusters of data points that are shared across devices . our approach uncovers a regularized linear predictor in an unsupervised manner that learns a causal graph around the unsupervised model . in addition to facilitating the unsupervised learning process , our regularization leads to an easier to interpret\n",
      "\n",
      "[1250 | 1186.86] loss=2.12 avg=2.07\n",
      "[1300 | 1415.32] loss=1.75 avg=2.01\n",
      "[1350 | 1643.71] loss=1.87 avg=1.99\n",
      "[1400 | 1871.89] loss=1.68 avg=1.95\n",
      "======== SAMPLE 1 ========\n",
      " which is often obtained from the case where n samples are drawn from a probability distribution p defined inside a kernel class h of size o . while this view does not necessarily lead to a better assessment of the relevance of the case to us , it provides a better view of the overall problem .\n",
      "in this paper , we propose a novel deep learning approach called recurrent nonparametric langevin mle for map inference . our approach integrates traditional parametric and discriminative approaches for map inference , improving scalability and interpretability of current ml approaches . using the recurrent nonparametric approach , we learn a nonparametric recurrent prior over the belief propagation function that maps the data into the inference state . this allows us to learn the policy and map for any given inference step , and we show that our method is accurate in a map dependent instance of map mixtures . empirically , we compare our approach with state of the art generative models on language modeling and pose estimation , which show state of the art improvements in accuracy and interpretability .\n",
      "the deep boltzmann machine is a deep generative model based on the map inference approach . we introduce a new version of the model based on the map inference approach , and apply it to deep belief networks , where map inference performs a state of the art job . however , the model is very deep because deep neural networks are embedded into the brain , and there is no way to exploit the embeddedness without it becoming too expensive . in this paper , we propose a neural encoding module that enables the use of the map inference approach for achieving high efficiency with very few extra parameters . in the encoding module , we embed arbitrary sets of filter parameters and inject them into the network to obtain a continuous filter mapping , thus enabling deep generative models . this approach is more affordable than current approaches where the parameter injection is expensive for every change to filter parameters . furthermore , by encoding our model using a low dimensional embedding that allows expansion of parameters at the edge of the space of relu functions , we can avoid expensive hyperparameter tuning . for example , by avoiding an expensive hyperparameter optimization scheme we can reduce the cost of relu functions by many orders of magnitude . experiments on two widely used datasets show that our approach is indeed faster than state of the art generative models and that our code is significantly easier to maintain .\n",
      "the task of clustering is strongly suggestive of machine learning . we start with approximate identifiability tests of the k layers of a deep network , which are the most popular testing method . we then choose a test formulation that allows us to express the clustering as an image clustering , which reduces the computational cost by several orders of magnitude . using popular image captioning , we find large clusters along different resolutions . our clustering results show a low variance across networks of lipsdeep and tensordeep architectures , which suggest an interest in studying clustering across computational scales .\n",
      "the recent success of deep learning has provided great advances in the study of large scale statistical and ontogenetic processes . such data include natural ontologies like insects and viral species . while such data should be treated with increasing trepidation , what can be done about it if we allow other internal processes , such as natural ontologies , to dominate our analysis and analyze data in a purely internal manner we show theoretically that it is necessary to allow internal processes within evolutionary biology to dominate our statistical analysis and analysis of large scale datasets . this leads to a new threshold for the statistical and ontogenetic weighting of sets of independent molecules based on their molecular weights . this threshold is used to speed up the discovery of new molecular families from observational data , in many cases within a day or so . furthermore , the resulting dataset is almost surely richer and broader than the one in which it was generated . finally , we experimentally verified the effectiveness of our threshold by demonstrating improvements in several ontogenetic datasets on the hm line animals visual and motor skills .\n",
      "variational autoencoders learn low dimensional , nonlinear representations that capture inter subject correlations in sensory and cognitive variables . such models have been shown to generalize well beyond the sensory genome , but thus require the representation of the covariance functions of neuronal responses to be arbitrary linear . in contrast , dldaestm takes into account the genetic programming that leads to the sparse estimation of the probabilistic graphical model underlying neuronal responses . here we propose a neural network approach to enable dldaestm to address the metabolic and genetic orders at individual levels of neurons without the use of endpoints on the prediction window . the architecture consists of two neural structures , a gaussian process model and a dlda recurrent neural circuit . the dmc generates neuronal responses from the posterior prior associated with the underlying neuronal models without the need of endpoints . the gp units are derived from the principles of maximum entropy theory . the dmc recurrent circuit is inspired by the recurrent u input model . endstate prediction using a network is augmented by conditional and nonlinear prediction policies . the modeling of the conditional and nonlinear predictions is robust based on the NUMBER saddle point theorem . we\n",
      "\n",
      "[1450 | 2117.68] loss=1.32 avg=1.88\n",
      "[1500 | 2345.97] loss=1.38 avg=1.83\n",
      "Saving checkpoint/run1/model-1500\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "[1550 | 2575.76] loss=1.37 avg=1.78\n",
      "[1600 | 2803.49] loss=1.20 avg=1.73\n",
      "======== SAMPLE 1 ========\n",
      " dynamics can be generalized to the case where each group of observations consists of a finite number of classes . if the learner can show that the distributions over observations match the corresponding class distributions on the fly , then generalized linear models are able to significantly improve the accuracy of the outputs of the group .\n",
      "this work introduces a formalism of functions that cannot be expressed in terms of functions and the restricted isometry setting aims to remedy this situation . we prove a second order reparameterization invariant optimization algorithm for such systems . the second order reparameterization invariant optimization algorithm can be combined with normalization to obtain a functional that is amenable to different optimization schemes . we further establish the consistency of a graded optimization procedure for this improper graded optimization problem . in particular , we resolve a non convex optimization problem where the convex limit depends on the rank of a function , and we illustrate the performance of this improper graded optimization .\n",
      "we provide sharp sharp finite contrast finite contrast finite comparison results for multi class classification . the results compare over class variates with convex relaxations while our result only applies to finite comparison relaxations . we establish an error bound of omega for convex relaxations that holds under any uniform relaxation consistency and vc support . as a by product we establish some generalization bounds for max force convergence . we introduce some new multi class classification results that are linearly consistent where the class contains only a fraction of the covariates .\n",
      "this manuscript describes new method and numerical analysis of multivariate linear models for matrices in a broad range of settings where the number of variables grows sublinearly with the factor size . it details four main results a novel dimension reduction for spatially structured matrices in taylor series is proposed and a technique for efficiently estimating the matrix dimension is introduced exploiting natural constraints the lasso is proposed , and convex dual averaging is also provided .\n",
      "the large margin markov chain monte carlo optimization of block splitting structured markov chains provides an efficient alternative to traditional structured stochastic gradient descent in many applications , such as state of the art efficient running time methods . although efficient sublinear speed algorithms are theoretically possible for the mcmc optimization , they incur prohibitive computational complexities for the traditional mcmc . we present methods that speed up mcmc optimization using cyclic oconcave methods , which operate by solving an alternating smooth network problem . computationally , oconcave methods use iteratively refitted rnn coordinate updates and lnns , respectively , to obtain a new efficient and flexible method . this new and faster implementation of the mcmc leads to a theoretically guaranteed convergence rate , the expected per iteration speed . therefore , both methods have broad applications in various markov chains with a common parameter , and also vanishing and exploding rates .\n",
      "we study the computational complexity of learning models with nonconvex loss functions and regularization strategies . we study two types a maximum likelihood approach that exploits the geometry of the nonconvex objective and a gradient descent method that explores the nonconvex hull in the parameter space . we analyze the difference in the parameters between the two approaches , which allows the former to learn much faster than the former . we examine the impact of different parameters in the parameter spaces when optimizing the latter .\n",
      "the success of deep learning and its potential applications have motivated research on learning from massive databases of labeled data . in particular , labeled data poses a significant privacy threat due to possible sparsity effects and the spread of errors . while there has been much work on recovering low rank models , there remains an underdeveloped understanding of the breakdown process between the training and the fine tuning process . we introduce a neural network as a simple model of nonlinear tuning and as a tool of recovery from errors from information diffusion . we introduce the asymptotic procedure for recovering a neural network from a realization of the ensemble training . we develop algorithms for computing parameters and proving error bounds for the class of collapsed unnormalized loss functions for which we present the asymptotic breakdown . when the error distribution is non linear in , our procedure recovers the class of collapsed unnormalized loss functions . by finding the asymptotic breakdown for gradients satisfying certain choice conditions , we obtain a new class of error recoveries for which we propose the first computationally efficient parameter free parameter recovery procedure .\n",
      "in this paper we present an algorithm for the partition principal component analysis . our approach relies on two main ideas max margin optimization of a cost function by use of heuristic thresholding techniques , and an explicitly index based learning process that combines a supervised learning task with a restricted ability learning task . our learning algorithm is highly efficient , requiring only about half the parameters of previous approaches and much less computational spend on the individual tasks . by integrating the proposed approach with probl , we obtain a new regularized version of the classical pca algorithm with NUMBER orders of magnitude fewer parameters . our evaluations demonstrate that our method is generally more accurate than comparable index based active learning methods and a similar working group effort .\n",
      "variational inference is a powerful abstraction of a variety\n",
      "\n",
      "[1650 | 3048.06] loss=1.38 avg=1.70\n",
      "[1700 | 3275.71] loss=1.17 avg=1.66\n",
      "[1750 | 3503.10] loss=1.32 avg=1.64\n",
      "[1800 | 3730.41] loss=1.09 avg=1.60\n",
      "======== SAMPLE 1 ========\n",
      " latent and the more common oracles , the more likely it is . we also introduce a new metric for measuring the risk of a novel novel discovery . for cifar NUMBER , we show the risk of a new discovery corresponding to asymptotically much larger risks than previously suggested , albeit only for small numbers of trials , suggesting that novel discoveries have very small risk than larger discoveries . we provide examples of novel discoveries compared to the standard criteria for having low risk .\n",
      "most previously studied clustering methods for the relational aspect of mls had to adapt their relational structure during the learning process to better capture the variation in human behavior . recent work in machine learning and statistics has improved the generalization accuracy of the model that is trained on the training data by adding appropriate hierarchical family relationships to the learning problem . the effectiveness of such models is somewhat disputable , particularly when the number of relational relationships in training data is large . given that statistical dependencies typically fall between the distribution of human judgments about the optimal cluster size , it remains unclear how to balance the benefits of modeling relational relationships and reduce the risk of individual mistakes in learning between pair . in this paper , we develop a framework which takes the form of a hierarchical clustering algorithm which ties together the clusters of observed data with probability relative accuracy in each cluster . we give theoretical guarantees for the quality of the resulting cluster inference algorithm . as a by product of our analysis , we also derive performance bounds for the implicit label aggregation in the latent structured model . finally , we experimentally demonstrate that our algorithm can learn accurate and effective semantic representations such as dialogs while using labels that are traditionally unseen in machine learning problems .\n",
      "we propose a framework for training deep neural networks on massive data represented as a graph . the graph originates from a specific training objective and is used by various networks to represent the training data . the graph is frequently used by downstream networks to train learning algorithms . theoretically , we prove that it minimizes the gradient of the gradient of the network with respect to the external gradient of the descent . empirically , we find that it does not significantly affect training accuracy . together , these results suggest that massive data training with massive external information improves training performance for a wide variety of reasons and converges at a linear rate with respect to the training weights .\n",
      "we propose a new reinforcement learning approach for guided and near guided combinatorial search in a setting where there is a monocular graph on which decisions can be made asynchronously . our approach maps the regret of combinatorial search to the dimension d of the objective function . this dimension allows to leverage the locality structure inherent in guided combinatorial optimization , to achieve better long term dependence on the random sampling strategy and lower estimation error of combinatorial search . it was shown in practice that guided combinatorial search has a computational complexity arbitrarily large for a general setting . the motivation behind our algorithm is likely related to sampling based sampling methods . as a proof of concept , we use empirical data gathered from user generated videos as well as an accident risk dataset collected on the internet .\n",
      "the problem of selecting the best arm in a stochastic mutli armed bandit problem is considered . we derive theoretical guarantees on regret and upper bounds on the optimal arm for the optimal loss function , which generalize results for the best case and worst case settings . in particular , we recover two terms that have been empirically shown to act as reserve terms for the experts , under the assumption that the network is fully autonomous . in contrast , we adopt the results of the present analysis for an operator generalisation that is , the expected regret for any fixed action selecting a single arm in a infinite multi step repeated game is reduced to o losses .\n",
      "many machine learning algorithms exhibit strong performance guarantees when used in conjunction with prediction markets . for instance , they provide performance guarantees if the traders accept the trade off and if the prices are competitive with the best offer . such strong performance can reflect a trade off for an asset that trade , we show , the trade off between the seller`s ability to generate liquidity and the amount of liquidity sold . our main result is a trade off for confidence that is greater than the excess risk of the best offer when the seller admits confidence that the best offer is stronger than the excess risk . we give these stronger performances when the buyers knows the order low . when the seller admits certainty that the best offer is below the mean , and when the buyers trades in confidence that is on the right hand side of the map . our main result is a better trade off for a prediction market that is near the seller and thus admits better guarantees in the longer run . we also show that , for polyhedral hedging , polyhedral design patterns can be learned that achieve even stronger and correspondingly better guarantees .\n",
      "this contribution deals with learning the conditional probability of an n dimensional gaussian m curve in only the case of outliers . it is motivated by the fact that , under such situations , a simple convex combination of basis functions yields predictive accuracy almost surely over various noise tolerance measures . it offers estimates of the noise variance in the estimates\n",
      "\n",
      "[1850 | 3974.55] loss=1.24 avg=1.58\n",
      "[1900 | 4200.81] loss=0.88 avg=1.54\n",
      "[1950 | 4427.97] loss=1.21 avg=1.52\n",
      "[2000 | 4655.98] loss=1.11 avg=1.50\n",
      "Saving checkpoint/run1/model-2000\n",
      "======== SAMPLE 1 ========\n",
      " independent of the data , yet we cannot fully exploit the data due to its high dimensionality . this has motivated our approach in the setting of multivariate gaussian processes . our key insights are as follows . first , prior art in defining norms based on higher order interactions is a fundamental problem for mab operations . second , the approach admits a principled trade off between the effort of sampling and its realization under inaccurate or even maliciously chosen labels . as a result , we have the ability to learn norms without the need for adaptive procedures . experimental results on real world data demonstrate the efficiency and effectiveness of the proposed approach .\n",
      "we develop a novel approach based on the maximum covariance estimating approach to stability theta bank . a key feature of our proposal is that , under optimal assumptions , each variable is invariant to the average covariance of the estimates for a fixed period , where the explanatory variables are known . we prove that the proposed approach is consistent enough to yield the recovery of a constant factor model , which under certain conditions ensures the stability of theta bank . we demonstrate the proposed method in several stability studies and analytically illustrate its performance .\n",
      "in this paper , we consider the difficult problem of finding contours and contours contours in uniformly perturbed subspace learning . a new resnet algorithm , called comp dl , is to build an algorithm that finds contour maps as the inverse of a perturbed subspace kernel over the ground truth . the proposed resnet algorithm builds on the previous smallest resnet algorithm , called resparse . we give a perturbation analysis of the proposed subspace kernel and show that its noise variance is not necessarily proportional to the square root of the noise level . we study the noise model parameter of resnet in order to find contour maps with exponential noise , i . e . , maps whose variance is even smaller than the noise level . the lower bounds are crucial for resnet to work and provide an asymptotical error upper bound for resnet , for which exact prediction is intractable . we provide experimental results on a real world web pages ranking problem , where resnet finds contour maps with exponential noise .\n",
      "we analyze the estimation of the log partition function of a large , convex structured distribution mathbbd . our results give new information about the size and shape of distributions that have exactly the same size and shape . furthermore , we give sharp regret bounds for this problem . we obtain several new convergence results for the problems discussed here .\n",
      "we present a new algorithm for solving nonconvex optimization problems . our algorithm is based on the idea of using a worst case to achieve the optimal convergence rate , where the worst case regret is the square root of the euclidean average of the cumulative empirical risks . our proposed algorithm uses the concept of average of empirical risks and does not use the fiedler delta of the e turn signal . we illustrate the algorithm by using it to solve two challenging sub tasks p minimizing an over complete linear model and n deciding which graph to output when evaluating the model for the first time .\n",
      "the problem of estimating a sparse vector from samples is central in many information retrieval tasks . it is also a less known but important concern , in particular in algebraic and statistical applications , that the size of the vector used to estimate the matrix be fixed . since it is more general and provable , we approximate this problem with a formula . the formula leads to several important generalization bounds , some of which we extend to the many cases where the underlying vector is sparse . we bound the conditional probability that a large number of observations is in error bounded only by the size of its base and the number of entries in the matrix . our formulation is interesting but straightforward , and provides an important functional point based on which to explore new solutions for this problem . we bound the conditional probability that the size of the vector used to estimate the error arising from sampling is bounded independently from the error arising from estimating the matrix with noisy samples . furthermore , while previous approaches assume assortative links between data locations , we prove that a first order approach to sparse estimation achieves bounds similar to the best in theory but without asymptotic error . this is accomplished by making use of properties of the sparsity of the data distribution . we illustrate our bounds and derive a novel form for joint coordinates for sparse estimation .\n",
      "we present a principled approach for learning from imitation lossy learning algorithms . one of the key challenges in traditional urns is the inherently high compression precision of huge storage matrices . on the other extreme , we tackle the problem using the learned domain loss , aiming to reduce the storage dimension by as much as possible . for strong and weak losses , we demonstrate the benefits of large bethe compression . for densities and strongly convex losses , large bethe compression is asymptotically asymptotically indistinguishable from the learned loss with an optimal constant compression ratio . for both cases , we give end to end improvement estimates based on megabyte rather than gigabyte storage , and show that larger bethe compressed instances are asympt\n",
      "\n",
      "[2050 | 4903.94] loss=0.82 avg=1.46\n",
      "[2100 | 5131.75] loss=0.82 avg=1.43\n",
      "[2150 | 5358.72] loss=0.90 avg=1.40\n",
      "[2200 | 5586.48] loss=0.91 avg=1.38\n",
      "======== SAMPLE 1 ========\n",
      " each of the sub modes at each iteration is complete . we show in experiments that our method outperforms all existing sub modes in piecewise linear time speed comparisons on a range of three real world tasks in part , and also achieve higher prediction accuracy on pascal voc NUMBER .\n",
      "this paper tackles the challenge of transferring knowledge in machine learning from a teacher . we present a double thompson sampling test for determining whether to consider a set of hypotheses of a given size . we formulate a double thompson matching task as a constrained high dimensional collaborative lifting problem . each member of the class is required to pick one hypothesis , which selects a subset hypothesis . the goal of the member of the class picks the hypothesis , which is not restricted by the size of the hypothesis , while the others in the class are asked to identify other hypotheses . the criterion used for selecting the hypotheses is defined as the extracted maximum agreement between the observed maximum and agreement of the class . the proposed paradigm combines the benefits of reduced classification accuracy and the potential to improve classification accuracy on a wide range of similarity measures .\n",
      "an increasing number of methods in science can represent represent and accomplish new kinds of relationships between data and their computational inputs . structured prediction has enjoyed remarkable success in modeling such relationships but now with integrated modeling tools , data processing can take advantage of temporally abstract features to speed up computation . to understand the capabilities of saliency detection in structured prediction , we analyze a particular form of bipartite graph cuts . we show that under mild conditions , when cut from the appropriate graph cut stream , exact saliency identification is achieved without too much cost . in the more complex regime where the graph is perfectly separated from the input into small squares , then approximate approximate exact saliency identification is achieved . an interesting aspect of our analysis is that the exact posterior on labels must be exact . we call the method showing finite junction conductance identification with partial graph partitioning the pinwheel of markov chains can be readily incorporated into existing structured prediction techniques . to connect our method with the broader prediction community , we invite your feedback . if you find our approach to be informative , or have additional insights or contributions how would you quantify the impact this method has on learning faster machine learning methods using simpler representations and less communication complexity per parameter learn more about each graph cut .\n",
      "it is well known that the brain receives only inputs from where far left . we analyze the decoding of neural populations in humans where rightarrow brain activity is recorded using optogenetic activity vector cells . we show that the firing patterns of neural populations in that population dependently trace back to a neural network , i . e . , the basis for firing rate models . our results provide a new derivation of spike based measures of neural variability and provide a new interpretation of the irreducible . a magnetic resonance imaging study is currently underway to tease apart the distributions of populations of optogenetic spike response neurons from various mnis . we relate our results with the recent work of koopman et al . , which showed separation of ventral and dorsal spiking activity patterns in a linear population model . we speculate that our results reveal a new , parsimonious , and unified hierarchy of neural computations and activity . furthermore , we show that the posterior distribution over neural firing rates is a more general representation of brain structure and activity than the structural equation . that the resulting hierarchy is signal propagation invariant . that the signal propagation patterns in dorsal , ventral , and glmy cortical populations are more similar than in bilateral orbit co ascent than in bilateral orbit eccentricity , and that the posterior mean squared error may be significantly smaller than the NUMBER error , we derive a sparse variant of the integral thatimates the error of the signal propagated through all parameters in the population . that this population uses a much less cluttered signal representation than that seen in bilateral orbit rescaled error can be accounted for by its much less cluttered signal representation . that the signal propagation patterns in glmi are more similar to the signal of a single individual than to multiple pathways gives rise to purposively reconstructed signal densities . that a multiscale population code may consist of signals with much higher precision than the signal of a single individual . that the signal selection for a population must Baal described multi pathway experiments accurately with minimal loss in signal modeling by the tune up of signal weights , and that signal modeling is intractable , Baal provides great accuracy guarantee in the face of such geometric consistency .\n",
      "we propose a new clustering technique , a parsimonious model with a novel well known proposal distribution , a simple count based method for inferring all the possible pairs of symbols in a binary stringentropic binary matrix , and a relu function for directly computing the number of possible pairwise symbols in the stringentropy score . the proposed approach is based on a reduction to the classical stochastic block model and its application to this new age of clustering techniques has been studied hitherto , largely focusing on its ill effects on the statistical properties of the classical stochastic block model . in the present paper , we present a new classical stoch\n",
      "\n",
      "[2250 | 5832.08] loss=0.68 avg=1.35\n",
      "[2300 | 6060.36] loss=0.72 avg=1.32\n",
      "[2350 | 6288.65] loss=0.56 avg=1.29\n",
      "[2400 | 6516.95] loss=0.51 avg=1.26\n",
      "======== SAMPLE 1 ========\n",
      " a new method that can estimate directly the change in the changesigns across the two sets . our approach uses two probability measures a change signal with a nearly matching parameter , and a lower bound on the difference between their values . at inference , both measures are computed without calculations under standard random forests . comparisons with methods based on pairwise information and neural networks , on the confidence in the predictions of the prediction model , and on the accuracy of our method are presented .\n",
      "the standard stationary global approach to scene classification uses the resnet architecture to achieve substantial gains in object recognition performance when trained on large datasets . however , the lack of an immediate usefulness inference could drastically reduce the sample complexity on new datasets , where d is the dimensionality of the data and n is the number of pixels . we consider difnet and build on top of the resnet architecture to obtain predictions that scale linearly with d . specifically , we propose a novel gaussian mixture model to learn hierarchical components to represent the degrees of freedom . this component is generated by recurrent neural networks that are trained end to end . we find that with sufficiently few training datasets , lgc allows for succinct descriptions of categories without using d for labels . on a new gc sample of up to NUMBER million ratings , we show significant improvements on previous state of the art object tracking models with a single test from resnet architecture .\n",
      "variational gaussian models minimize confounders in the estimation of latent variable descriptors using a form of community estimation . variational gaussian models relax the statistical dependence on communities so that they can be efficiently estimated through stochastic optimization . we define a broad and consistent framework for representing the variational behavior of gaussian models through nonparametric bayesian optimization of the log likelihood . bayesian estimation uses copulas with long consecutive consecutive variances , which define the grounding of the fourier relations between samples . using acb is data augmentation , i . e . , taking samples from each density and adding the old samples with the new ones , which is typically not available in traditional variational gaussian models . experimental evidence suggests that the new model can be considerably more efficient than the standard gvi . the former typically involves a varied update procedure where the community prior is approximated by a weighted log likelihood function , whereas the latter involves a weighted likelihood bootstrap , which is characterized in many ways but primarily depends on the varied log likelihood function . we describe such procedures for emph and gms , and show with experiments that the new rgd reduces to operating in a non parametric bayesian framework in a large extent , and the varied log likelihood bootstrap provides a computationally efficient alternative to alative .\n",
      "we investigate a design principle that admits fast implementation of standard algorithms for floating point applications . the resulting algorithm selects a point subset and sets the parameters in a manner that is semantically meaningful . we prove a finite sample analysis of this approach cite , and obtain a tail convergence rate in submodular set regression . our experiments lend support to the theory , and provide insights on design principles and design strategies for a range of problems in machine learning .\n",
      "we develop a kernel approximation based method for bayesian learning with dependence fixed points for efficient exact evaluation . the algorithm is based on a carefully designed stochastic gradient method , referred to as carefully designed knn for short dot products , which was inspired by the well known well known geodesic property of the sum product effect of two vectors . we derive isotonic regression risk bounds of o , where k is the number of units to be expanded . our technique results in a significantly smaller variance compared to isotonic regression for regression , both for k factors , and for n and m . we corroborate these results through extensive simulations and an empirical evaluation of tractable models for both discrete and discrete systems .\n",
      "in the neural network literature , the leaky relus appear in a binary classifying problem , often corresponding to tight nonconvex optimization . however , existing studies on the coding and peak logs of these relus are focused on testing if there is a linear relationship between these quantities , thus they do not fully capture the distribution of the network flows . in this paper , we examine the impact of link estimation on estimating the family wise positive relus . our main result is a logarithmic factor by linearly specifying which units there are negative arrivals . it predicts the response feature vectors associated with our solution including pair wise dependence , positive skew , stationarity , and response term dependence . by leveraging this prior information , we develop an unbiased estimator , termed as a leaky relus . from this simple result , we derive a non asymptotic framework to directly quantify the quality of solutions for learning by interactive clustering , and from a spectral perspective , we derive an unbiased heuristic for optimizing the network log likelihood .\n",
      "for transferring data across devices , the k means objective captures high sample complexity and is generalized to equivalence terms for both loss and transfer costs . in this work we introduce a transfer method based on cluster descent directions for k means . we apply this gradient\n",
      "\n",
      "[2450 | 6762.02] loss=0.70 avg=1.23\n",
      "[2500 | 6988.97] loss=0.47 avg=1.20\n",
      "Saving checkpoint/run1/model-2500\n",
      "[2550 | 7219.67] loss=0.30 avg=1.17\n",
      "[2600 | 7447.85] loss=0.44 avg=1.14\n",
      "======== SAMPLE 1 ========\n",
      " majority of methods , such as cfrn . however , cfrn is typically treated as an empirical bayesian problem and the analysis is performed under the assumption that the random samples will always belong to the optimal class . such assumptions are not reasonable in practical settings . in this paper , we propose a more restrictive model called implicit distribution free learning , which estimates the distribution of interest and hence avoids the need for explicit estimation of the likelihood . in cfrn , we do not make explicit the assumption that the data have a discrete form . we study the properties of cfrn and compare it to some other widely used probabilistic models . we also study the properties of the posterior for the gaussian process . finally , we evaluate cfrn and cfrn , and show that in comparison to other widely used algorithms , cfrn is consistently more accurate and has a lower computational cost than other common gaussian process models .\n",
      "we propose a novel adaptive sampling approach to resnets for the benefit of learning quickly and accurately . in particular , we show improved prediction performance on three datasets under different environmental scenarios over multiple test epochs with automatic generation methods , without the need for parameter tuning . in addition , we provide a comprehensive synthetic and real world demonstration of the strength of the predictions . we demonstrate that the techniques applicable to our scenario achieve an accuracy on the large majority of simulated and real datasets .\n",
      "many structured prediction problems can be solved by training a graph neural network to model the input signal . structured output models , such systems rely on the ability to model similarity in paired or associable pairs of components for high class probability values such as the professor prediction ratio obtained by lans or bandit problems . however , it is usually not possible to accurately model pairwise similarity between components in the structured output model due to the high computational cost of cross validation . in this work , we propose a novel pairwise structured prediction algorithm , coupled with signal conditioning , that can exploit pairwise pairwise correlation to estimate model parameters . we demonstrate that the proposed method achieves not only the optimal pairwise similarity but also an optimal pairwise similarity estimator on test epoch pairs from well known pairwise structured prediction methods , such as spike and graph predictors . furthermore , the proposed method achieves very good performance on real world datasets with well known model parameters such as cut block cut block coding precision and reinforce block reinforce which greatly reduces model complexity in training .\n",
      "we propose a simple and efficient approach to learning stochastic gradients in the presence of noisy observations , completely avoiding the exhaustive search procedure common in machine learning . moreover , our approach allows for flexible optimization strategies that can be discoverable an efficient implementation of style ggms on training datasets has recently been proposed by zhang et al . and we report on it in the most significant deployment of noisy observations yet . the key ingredient of the deployment is a benchmark example learning algorithm applied to at least ten distinct medical diagnosis logs over a three month period , on natural images . this comparison demonstrates that the static collection method of hat ml , applied to training data from a neuro imaging dataset , achieves on par or better with various methods the relative efficiency of distribution hat ml , relative load balance and relative load availability .\n",
      "we consider the problem of binary classication on trust structured trust securities . the problem consists of betting on a common class and risk a maker against a ceiling on a one class budget . in this setting , the risk is revealed to the bettor via one of three different means one , given the provenance of the common class , positivity to positivity of the bettor offering and a resolution of the budget risk . the problem requires learning of non negative numbers of draws to guarantee that the risk is allocated to the most money than the budget is guaranteed to . our work is motivated by a potential use as the underlying budgeting method for statistical models . however , we do not fully exploit the basic structure of the problem and thus our approach imposes structure constraints that may restrict its applicability . we validate our approach in two real world adversarial applications the challenging task of detecting corruption in a social network data set generated from the head of an anti fraud unit has a high activity potential , whereas a random internet user generated dataset generated from a malicious site has a low potential activity . in both cases , the risk is not super linear and the performance could be better controlled adversarially . our approach , called bios dia NUMBER , achieves the optimal risk norm NUMBER minimizing the total variation energy cost NUMBER and maximally improving the overall performance .\n",
      "motivated on account of the complexity of its learners , the norm of deviation of an unknown function from its max sum counterpart in em may be efficiently computed by linear programming . motivated by the optimizational nature of the max sum optimization problem , this paper develops a fast and simple method for the calculation of the new frobenius norm . the paper proves that the new norm lies on the neir metric , which amounts to an approximate orthogonal regression on a smooth function . empirical results are presented to support the theoretical results .\n",
      "a new method for learning to solve\n",
      "\n",
      "[2650 | 7694.01] loss=0.39 avg=1.12\n",
      "[2700 | 7922.27] loss=0.44 avg=1.09\n",
      "[2750 | 8150.86] loss=0.53 avg=1.07\n",
      "[2800 | 8379.44] loss=0.52 avg=1.06\n",
      "======== SAMPLE 1 ========\n",
      " different method , when the input points are weighted using a random , suboptimizable , or old world version of the inverse hessian we call the resulting weights are random . in this paper , we focus on the multiple application setting , where prior knowledge , input , and output vectors can be applied linearly in an asynchronous and communication constrained manner . we formalize this prior gaussian process implicitly in the posterior distribution , and show that a single bound on the weights can be obtained by sampling backwards through time in an approximate manner . this posterior distribution free prior is not only able to characterize the scaling behavior of different tasks concurrently , but also results in smooth surrogates for handling bias and discrimination if little prior knowledge is taken into account . remarkably , the resulting multiple application preference variable result is significantly parallelizable and is robust to outliers it provides , unlike the standard kl approximation , is of independent interest .\n",
      "given an n x n pairwise matrix a , identifying a query vector x rightarrow m is crucial to have accurate answers possible in importantpopulations over several words , even in the presence of missing data . this paper proposes a novel n x n decomposition of the incomplete inverse matrix m part of a dense , high dimensional natural speech document , called the exemplar , into two components a coherent exemplar denoising step , and an elimination step . in addition to form , the coding and summarizing step implement the natural semantics of the exemplar to enhance the speaker embedding and the speaker embedding information . the end result is a data driven coding and summarizing step that is also relatively efficient at training since it can find a sparse set of topics in a document of substantially higher dimension than its density relative to the ambient document . we evaluate the results on several applications real and synthetic and report encouraging results that show the complexity of the problem indeed goes beyond such things , improving the baseline approach of least probabilistic .\n",
      "the lasso and elastic net linear regression models provide a convenient way to incorporate features of individual trials to infer risk functions from measured data . despite the important theoretical importance , fitting the lasso and elastic net to a data set is now simultaneously the object of many applications , news items wearers task , students at school level , and year rounders . we consider an application in which the difficulty is to estimate risk function dependently from limited data for the task of lasso estimation in finite representations . we focus on the task of lasso prediction and outlier detection with time scaling penalty . in particular , we introduce a novel hierarchical , multi scale formulation of predictors and our analysis shows that the proposed lasso and elastic net can be regarded as autoencoders with risk estimators in two different domains . we show that the proposed autoencoders can be successfully used in a data dependent learning setting . the efficiency of the proposed methods is demonstrated in experiments .\n",
      "in this paper , we propose a scalable algorithm for submodular minimization which is based on the projection formulation , but which , forbid to be in a class , submodular minimization , where the set of components is a bag of matchings , with a potential dimension d . our algorithm is simple and easy to implement . the single invocation of shapley included an order o algorithm for a width of omega and a depth ot that is , surprisingly , adaptive . we show that such adaptive algorithms exhibit submodularity to avoid subtime and time complexity in submodular minimization that is , a subtime and time complexity due to the projection formulation need o times o . we complement our analysis with an empirical evaluation of an adaptively specified algorithm , which matches the performance of the optimal submodular solution .\n",
      "in stochastic optimal control the distribution of the exogenous noise depends on a set of expectations that also indicates the age of the current regime . the best known of these is the sigma stochastic variance reduction algorithm , which generalizes the widely used sesamp and celeba convergences . most works have focused on the estimation of the sigmoid approximation to the log likelihood . we propose a new algorithm , namely a time and sample complexity free sesamp , that approaches the sesamp and sesamp by considering and solving a weighted binary classification problem . our algorithm is in essence a time and sample complexity generalization of the recently proposed inlier smoothing which can be solved in a neural network framework . we show that the algorithm leads to significantly better estimates of the posterior distribution with far fewer hyperparameters than sparse sesam with NUMBER ms training and an accuracy of NUMBER in a cubic second with respect to the log likelihood .\n",
      "we propose a new formulation for active learning in which each learner chooses an action in a round and observes the outcomes of all chosen actions in an appropriate order . this action alignment problem is one of the most important alignment problems in machine learning . in particular , it is equally important in unsupervised latent dirichlet allocation or feedforward network learning . to address this issue , we consider a weakly supervised version of the active learning problem\n",
      "\n",
      "[2850 | 8624.76] loss=0.36 avg=1.03\n",
      "[2900 | 8853.10] loss=0.29 avg=1.01\n",
      "[2950 | 9081.51] loss=0.35 avg=0.99\n",
      "[3000 | 9309.76] loss=0.24 avg=0.97\n",
      "Saving checkpoint/run1/model-3000\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name='124M',\n",
    "              steps=1000,\n",
    "              restore_from='latest',\n",
    "              run_name='run1',\n",
    "              print_every=50,\n",
    "              sample_every=200,\n",
    "              save_every=500,\n",
    "              overwrite=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVLiaAS-rVRj"
   },
   "outputs": [],
   "source": [
    "# copy the checkpoint folder to your own Google Drive\n",
    "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gpt2_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
